{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of Assignment-1-part1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nKa3aWguk5hF",
        "t7Rs_Tmuk5hU",
        "BgKTHbhQk5hW",
        "Q6ODyeevk5hb"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-visionary/assignment1/blob/main/Copy_of_Assignment_1_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-OdbdwvgLwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f966619c-df95-4385-ef42-f254a44b0783"
      },
      "source": [
        "# Steps to load extra files and download dataset.\r\n",
        "!git clone \"https://pradum22:143%40simplyGIT@github.com/the-visionary/assignment1.git\"\r\n",
        "import os\r\n",
        "os.chdir('/content')\r\n",
        "os.chdir('assignment1')\r\n",
        "os.chdir('assignment1-part1')\r\n",
        "assert (os.getcwd() == '/content/assignment1/assignment1-part1')\r\n",
        "\r\n",
        "os.chdir('cifar10')\r\n",
        "%run get_datasets.py\r\n",
        "os.chdir('..')\r\n",
        "assert (os.getcwd() == '/content/assignment1/assignment1-part1')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'assignment1'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 56 (delta 18), reused 32 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (56/56), done.\n",
            "Downloading...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-U6y_Q48SkQ"
      },
      "source": [
        "import os\r\n",
        "os.chdir('/content/assignment1/assignment1-part1')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnLDs1Qjk5go"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from data_process import get_CIFAR10_data, get_MUSHROOM_data\n",
        "from scipy.spatial import distance\n",
        "import models\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdkCgEF_7Si9"
      },
      "source": [
        "import importlib\r\n",
        "import sys\r\n",
        "importlib.reload(sys.modules['models'])\r\n",
        "from models import SVM\r\n",
        "\r\n",
        "# importlib.reload(models.SVM)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKa3aWguk5hF"
      },
      "source": [
        "# Loading CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKjr6ySOk5hH"
      },
      "source": [
        "In the following cells we determine the number of images for each split and load the images.\n",
        "<br /> \n",
        "TRAIN_IMAGES + VAL_IMAGES = (0, 50000]\n",
        ", TEST_IMAGES = 10000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y38ztIRbk5hI"
      },
      "source": [
        "# You can change these numbers for experimentation\n",
        "# For submission we will use the default values \n",
        "TRAIN_IMAGES = 40000\n",
        "VAL_IMAGES = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7vKnzYVk5hJ"
      },
      "source": [
        "data = get_CIFAR10_data(TRAIN_IMAGES, VAL_IMAGES)\n",
        "X_train_CIFAR, y_train_CIFAR = data['X_train'], data['y_train']\n",
        "X_val_CIFAR, y_val_CIFAR = data['X_val'], data['y_val']\n",
        "X_test_CIFAR, y_test_CIFAR = data['X_test'], data['y_test']\n",
        "n_class_CIFAR = len(np.unique(y_test_CIFAR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8pQ9apak5hJ"
      },
      "source": [
        "Convert the sets of images from dimensions of **(N, 3, 32, 32) -> (N, 3072)** where N is the number of images so that each **3x32x32** image is represented by a single vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W__jV7mk5hK"
      },
      "source": [
        "X_train_CIFAR = np.reshape(X_train_CIFAR, (X_train_CIFAR.shape[0], -1))\n",
        "X_val_CIFAR = np.reshape(X_val_CIFAR, (X_val_CIFAR.shape[0], -1))\n",
        "X_test_CIFAR = np.reshape(X_test_CIFAR, (X_test_CIFAR.shape[0], -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDTU_J2Fk5hK"
      },
      "source": [
        "# Loading Mushroom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfFsQz-rk5hL"
      },
      "source": [
        "In the following cells we determine the splitting of the mushroom dataset.\n",
        "<br /> TRAINING + VALIDATION = 0.8, TESTING = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrtQP5rmk5hL"
      },
      "source": [
        "# TRAINING = 0.6 indicates 60% of the data is used as the training dataset.\n",
        "VALIDATION = 0.2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR9SLJmGk5hM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a93557-416a-4d71-a022-7f7ee3c0de34"
      },
      "source": [
        "data = get_MUSHROOM_data(VALIDATION)\n",
        "X_train_MR, y_train_MR = data['X_train'], data['y_train']\n",
        "X_val_MR, y_val_MR = data['X_val'], data['y_val']\n",
        "X_test_MR, y_test_MR = data['X_test'], data['y_test']\n",
        "n_class_MR = len(np.unique(y_test_MR))\n",
        "\n",
        "print(\"Number of train samples: \", X_train_MR.shape[0])\n",
        "print(\"Number of val samples: \", X_val_MR.shape[0])\n",
        "print(\"Number of test samples: \", X_test_MR.shape[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train samples:  4874\n",
            "Number of val samples:  1625\n",
            "Number of test samples:  1625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ChNyW3vLZlh"
      },
      "source": [
        "y_train_MR = y_train_MR.reshape(y_train_MR.shape[0], 1)\r\n",
        "y_test_MR = y_test_MR.reshape(y_test_MR.shape[0], 1)\r\n",
        "y_val_MR = y_val_MR.reshape(y_val_MR.shape[0], 1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ6yT5JWk5hN"
      },
      "source": [
        "### Get Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFYEceHAk5hN"
      },
      "source": [
        "This function computes how well your model performs using accuracy as a metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qEcgIejk5hN"
      },
      "source": [
        "def get_acc(pred, y_test):\n",
        "  print(\"test\",y_test,\"pred\",pred)\n",
        "  return np.mean(y_test==pred)*100"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHnTDc1CzH0E",
        "outputId": "09c762ba-2a33-4de3-fda2-b626c1451309"
      },
      "source": [
        "X_train_CIFAR.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 3072)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7Rs_Tmuk5hU"
      },
      "source": [
        "# Support Vector Machines (with SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n03H1uHBk5hU"
      },
      "source": [
        "First, you will implement a \"soft margin\" SVM. In this formulation you will maximize the margin between positive and negative training examples and penalize margin violations using a hinge loss.\n",
        "\n",
        "We will optimize the SVM loss using SGD. This means you must compute the loss function with respect to model weights. You will use this gradient to update the model weights.\n",
        "\n",
        "SVM optimized with SGD has 3 hyperparameters that you can experiment with :\n",
        "- **Learning rate** - similar to as defined above in Perceptron, this parameter scales by how much the weights are changed according to the calculated gradient update. \n",
        "- **Epochs** - similar to as defined above in Perceptron.\n",
        "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case it is a coefficient on the term which maximizes the margin. You could try different values. The default value is set to 0.05."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V52Dp--Hk5hV"
      },
      "source": [
        "*You* will implement the SVM using SGD in the **models/SVM.py**\n",
        "\n",
        "The following code: \n",
        "- Creates an instance of the SVM classifier class \n",
        "- The train function of the SVM class is trained on the training data\n",
        "- We use the predict function to find the training accuracy as well as the testing accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgKTHbhQk5hW"
      },
      "source": [
        "## Train SVM on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LTIci5kxwc2",
        "outputId": "342cd1f2-e910-4d3d-f6a2-373abdba9d2e"
      },
      "source": [
        "print(n_class_CIFAR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mWwjaG1k5hW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "08931741-79b2-4745-a0a8-0f2efaa44421"
      },
      "source": [
        "lr = 0.01\n",
        "n_epochs = 100\n",
        "reg_const = 0.05\n",
        "\n",
        "svm_CIFAR = SVM(n_class_CIFAR, lr, n_epochs, reg_const)\n",
        "svm_CIFAR.train(X_train_CIFAR, y_train_CIFAR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-2004e080d81f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreg_const\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msvm_CIFAR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_class_CIFAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_const\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msvm_CIFAR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_CIFAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_CIFAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_class_CIFAR' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2mXPO1Uk5hW"
      },
      "source": [
        "pred_svm = svm_CIFAR.predict(X_train_CIFAR)\n",
        "print('The training accuracy is given by: %f' % (get_acc(pred_svm, y_train_CIFAR)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOASzOOak5hW"
      },
      "source": [
        "### Validate SVM on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKiIBnzTk5hX"
      },
      "source": [
        "pred_svm = svm_CIFAR.predict(X_val_CIFAR)\n",
        "print('The validation accuracy is given by: %f' % (get_acc(pred_svm, y_val_CIFAR)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm3DeY1rk5hX"
      },
      "source": [
        "### Test SVM on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGOKQIs-k5hX"
      },
      "source": [
        "pred_svm = svm_CIFAR.predict(X_test_CIFAR)\n",
        "print('The testing accuracy is given by: %f' % (get_acc(pred_svm, y_test_CIFAR)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyIM8cs8k5hY"
      },
      "source": [
        "## Train SVM on Mushroom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bo-b15r_FGM"
      },
      "source": [
        "\"\"\"Support Vector Machine (SVM) model.\"\"\"\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from random import random\r\n",
        "from sklearn.utils import shuffle\r\n",
        "\r\n",
        "\r\n",
        "class SVM:\r\n",
        "    def __init__(self, n_class: int, lr: float, epochs: int, reg_const: float):\r\n",
        "        \"\"\"Initialize a new classifier.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            n_class: the number of classes\r\n",
        "            lr: the learning rate\r\n",
        "            epochs: the number of epochs to train for\r\n",
        "            reg_const: the regularization constant\r\n",
        "        \"\"\"\r\n",
        "        # Size same as X\r\n",
        "        self.w = None  # TODO: change this: OK\r\n",
        "        self.alpha = lr\r\n",
        "        self.epochs = epochs\r\n",
        "        self.reg_const = reg_const\r\n",
        "        self.n_class = n_class\r\n",
        "\r\n",
        "    def distances(self, X_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\r\n",
        "        N = X_train.shape[0]\r\n",
        "        y_train.reshape((N,1))\r\n",
        "        dist = 1 - y_train * (np.dot(X_train, self.w))\r\n",
        "        dist[dist < 0] = 0\r\n",
        "        return dist\r\n",
        "\r\n",
        "    def cost(self, X_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\r\n",
        "        N = X_train.shape[0]\r\n",
        "        dist = self.distances(X_train, y_train)\r\n",
        "        hinge_loss = 1/reg_const * np.sum(dist) / N\r\n",
        "        cost = 1/2 * np.dot(self.w.T, self.w) + hinge_loss\r\n",
        "        return cost\r\n",
        "\r\n",
        "    def calc_gradient(self, X_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\r\n",
        "        \"\"\"Calculate gradient of the svm hinge loss.\r\n",
        "\r\n",
        "        Inputs have dimension D, there are C classes, and we operate on\r\n",
        "        mini-batches of N examples.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            X_train: a numpy array of shape (N, D) containing a mini-batch\r\n",
        "                of data\r\n",
        "            y_train: a numpy array of shape (N,) containing training labels;\r\n",
        "                y[i] = c means that X[i] has label c, where 0 <= c < C\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            the gradient with respect to weights w; an array of the same shape\r\n",
        "                as w\r\n",
        "        \"\"\"\r\n",
        "        N = X_train.shape[0]\r\n",
        "        cost = self.cost(X_train, y_train)\r\n",
        "\r\n",
        "        dw = np.zeros(self.w.shape)\r\n",
        "        # print('dw ', dw.shape)\r\n",
        "        dist = self.distances(X_train, y_train)\r\n",
        "        # print('dist ', dist.shape)\r\n",
        "\r\n",
        "        for i, d in enumerate(dist):\r\n",
        "            d = d.item()\r\n",
        "            # print('d ', d.shape)\r\n",
        "            # print('w ', self.w.shape)\r\n",
        "            # print('y ', y_train[i].shape)\r\n",
        "            # print('x ', X_train[i].shape)\r\n",
        "            x = X_train[i].reshape(X_train.shape[1], 1)\r\n",
        "            y = y_train[i].item()\r\n",
        "            if max(0, d) == 0:\r\n",
        "                di = self.w\r\n",
        "            else:\r\n",
        "                di = self.w - (self.reg_const * y) * x\r\n",
        "            dw += di\r\n",
        "\r\n",
        "        dw = dw / N\r\n",
        "        return dw\r\n",
        "\r\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\r\n",
        "        \"\"\"Train the classifier.\r\n",
        "\r\n",
        "        Hint: operate on mini-batches of data for SGD.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            X_train: a numpy array of shape (N, D) containing training data;\r\n",
        "                N examples with D dimensions\r\n",
        "            y_train: a numpy array of shape (N,) containing training labels\r\n",
        "        \"\"\"\r\n",
        "        N = X_train.shape[0]\r\n",
        "        x_size = X_train.shape[1]\r\n",
        "        self.w = np.array([random()] * x_size).reshape(x_size, 1) # Initialize random weights\r\n",
        "        # print('W', self.w.shape)\r\n",
        "\r\n",
        "        # grad = self.calc_gradient(X_train, y_train)\r\n",
        "        for epoch in range(1, self.epochs + 1):\r\n",
        "            X, Y = shuffle(X_train, y_train)\r\n",
        "            Y = Y.reshape((y_train.shape[0], 1))\r\n",
        "            grad = self.calc_gradient(X, Y)\r\n",
        "            self.w = self.w - (self.alpha * grad)\r\n",
        "            cost = self.cost(X_train, y_train)\r\n",
        "        print(self.w)\r\n",
        "        return\r\n",
        "\r\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\r\n",
        "        \"\"\"Use the trained weights to predict labels for test data points.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            X_test: a numpy array of shape (N, D) containing testing data;\r\n",
        "                N examples with D dimensions\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            predicted labels for the data in X_test; a 1-dimensional array of\r\n",
        "                length N, where each element is an integer giving the predicted\r\n",
        "                class.\r\n",
        "        \"\"\"\r\n",
        "        # print(np.sign(X_test @ self.w).shape)\r\n",
        "        y = X_test.dot(self.w)\r\n",
        "        y_pred = np.sign(y)\r\n",
        "        return y_pred\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMIjxD8Dk5hY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56c1bfb-c35c-40f2-d950-337e6ea255dd"
      },
      "source": [
        "lr = 0.01\n",
        "n_epochs = 500\n",
        "reg_const = 0.05\n",
        "\n",
        "svm_MR = SVM(n_class_MR, lr, n_epochs, reg_const)\n",
        "# print(svm_MR.cost(X_train_MR, y_train_MR))\n",
        "svm_MR.train(X_train_MR, y_train_MR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.00453611]\n",
            " [ 0.01446943]\n",
            " [-0.00163722]\n",
            " [-0.00570398]\n",
            " [-0.00900503]\n",
            " [ 0.00615319]\n",
            " [ 0.00024687]\n",
            " [ 0.01761931]\n",
            " [-0.08301618]\n",
            " [ 0.00296991]\n",
            " [-0.01396339]\n",
            " [-0.00459187]\n",
            " [-0.0050228 ]\n",
            " [-0.0140893 ]\n",
            " [-0.01416115]\n",
            " [ 0.00654776]\n",
            " [ 0.00547461]\n",
            " [ 0.00255625]\n",
            " [-0.02991872]\n",
            " [ 0.02025357]\n",
            " [ 0.01998891]\n",
            " [ 0.02141483]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiG_ksW8k5hY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a732f88-bb10-42cd-f1e5-40b7ea38af1b"
      },
      "source": [
        "pred_svm = svm_MR.predict(X_train_MR)\n",
        "print('The training accuracy is given by: %f' % (get_acc(pred_svm, y_train_MR)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training accuracy is given by: 71.932704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm2vYxt3k5hZ"
      },
      "source": [
        "### Validate SVM on Mushroom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhAqOUOOk5hZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfdbc94c-4d28-4019-f180-448ac2ce5e02"
      },
      "source": [
        "pred_svm = svm_MR.predict(X_val_MR)\n",
        "print('The validation accuracy is given by: %f' % (get_acc(pred_svm, y_val_MR)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The validation accuracy is given by: 71.200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLVybR1nk5hZ"
      },
      "source": [
        "## Test SVM on Mushroom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnhjeUg4k5hZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21789ca8-43c9-4b22-c771-7983f76a5993"
      },
      "source": [
        "pred_svm = svm_MR.predict(X_test_MR)\n",
        "print('The testing accuracy is given by: %f' % (get_acc(pred_svm, y_test_MR)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The testing accuracy is given by: 72.984615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39BwwwJ2k5ha"
      },
      "source": [
        "# Softmax Classifier (with SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "G9LY2K_Ek5ha"
      },
      "source": [
        "Next, you will train a Softmax classifier. This classifier consists of a linear function of the input data followed by a softmax function which outputs a vector of dimension C (number of classes) for each data point. Each entry of the softmax output vector corresponds to a confidence in one of the C classes, and like a probability distribution, the entries of the output vector sum to 1. We use a cross-entropy loss on this sotmax output to train the model. \n",
        "\n",
        "Check the following link as an additional resource on softmax classification: http://cs231n.github.io/linear-classify/#softmax\n",
        "\n",
        "Once again we will train the classifier with SGD. This means you need to compute the gradients of the softmax cross-entropy loss function according to the weights and update the weights using this gradient. Check the following link to help with implementing the gradient updates: https://deepnotes.io/softmax-crossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXF5m_B4k5ha"
      },
      "source": [
        "The softmax classifier has 3 hyperparameters that you can experiment with :\n",
        "- **Learning rate** - As above, this controls how much the model weights are updated with respect to their gradient.\n",
        "- **Number of Epochs** - As described for perceptron.\n",
        "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case, we minimize the L2 norm of the model weights as regularization, so the regularization constant is a coefficient on the L2 norm in the combined cross-entropy and regularization objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwApQ5UWk5ha"
      },
      "source": [
        "You will implement a softmax classifier using SGD in the **models/Softmax.py**\n",
        "\n",
        "The following code: \n",
        "- Creates an instance of the Softmax classifier class \n",
        "- The train function of the Softmax class is trained on the training data\n",
        "- We use the predict function to find the training accuracy as well as the testing accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6ODyeevk5hb"
      },
      "source": [
        "## Train Softmax on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VCXon4WCFSe"
      },
      "source": [
        "\"\"\"Softmax model.\"\"\"\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "class Softmax:\r\n",
        "    def __init__(self, n_class: int, lr: float, epochs: int, reg_const: float):\r\n",
        "        \"\"\"Initialize a new classifier.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            n_class: the number of classes\r\n",
        "            lr: the learning rate\r\n",
        "            epochs: the number of epochs to train for\r\n",
        "            reg_const: the regularization constant\r\n",
        "        \"\"\"\r\n",
        "        self.lr = lr\r\n",
        "        self.epochs = epochs\r\n",
        "        self.reg_const = reg_const\r\n",
        "        self.n_class = n_class\r\n",
        "        self.w = np.random.randn(self.n_class,3072)\r\n",
        "\r\n",
        "    def calc_gradient(self, X_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\r\n",
        "        \"\"\"Calculate gradient of the softmax loss.\r\n",
        "\r\n",
        "        Inputs have dimension D, there are C classes, and we operate on\r\n",
        "        mini-batches of N examples.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            X_train: a numpy array of shape (N, D) containing a mini-batch\r\n",
        "                of data\r\n",
        "            y_train: a numpy array of shape (N,) containing training labels;\r\n",
        "                y[i] = c means that X[i] has label c, where 0 <= c < C\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            gradient with respect to weights w; an array of same shape as w\r\n",
        "        \"\"\"\r\n",
        "        #Calculating Cross entropy loss\r\n",
        "        print(\"\\nweights: \",self.w)\r\n",
        "        x = X_train\r\n",
        "        z = np.dot(self.w, x) # 10x3072 - 3072x40000 = 10x40000(z)\r\n",
        "        # print(\"\\noriginal z\", z)\r\n",
        "        z -= np.max(z, axis=0) \r\n",
        "        print(\"\\nchanged z\",z[0])\r\n",
        "        p_sum = np.sum(np.exp(z), axis=0)\r\n",
        "        p = np.exp(z) / p_sum # Softmax function 10x40000(p)\r\n",
        "        print(\"\\np = \" ,p, p_sum)\r\n",
        "        L = -1/len(y_train) * np.sum(np.log(p[y_train, range(len(y_train))])) #Cross entropy loss\r\n",
        "        R = 0.5 * np.sum(np.multiply(self.w, self.w))\r\n",
        "        loss = L + R * reg_const\r\n",
        "\r\n",
        "        #Calculation of Grad\r\n",
        "        p[y_train, range(len(y_train))] -= 1\r\n",
        "        dW = (1 / len(y_train)) * p.dot(x.T) + (reg_const * self.w)\r\n",
        "        return loss, dW\r\n",
        "\r\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\r\n",
        "        \"\"\"Train the classifier.\r\n",
        "\r\n",
        "        Hint: operate on mini-batches of data for SGD.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            X_train: a numpy array of shape (N, D) containing training data;\r\n",
        "                N examples with D dimensions\r\n",
        "            y_train: a numpy array of shape (N,) containing training labels\r\n",
        "        \"\"\"\r\n",
        "        x = X_train.T\r\n",
        "        N = x.shape[1]\r\n",
        "        loss_calc = []\r\n",
        "        for itr in range(1, self.epochs+1):\r\n",
        "          loss, grad = self.calc_gradient(x, y_train)\r\n",
        "          loss_calc.append(loss)\r\n",
        "          print(f\"\\nLoss in epoch {itr} is {loss}\")\r\n",
        "          # print(f\"\\nGrad in epoch {itr} is {grad}\")\r\n",
        "          self.w -= self.lr * grad \r\n",
        "        print(self.w)\r\n",
        "        return\r\n",
        "\r\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\r\n",
        "        \"\"\"Use the trained weights to predict labels for test data points.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            X_test: a numpy array of shape (N, D) containing testing data;\r\n",
        "                N examples with D dimensions\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            predicted labels for the data in X_test; a 1-dimensional array of\r\n",
        "                length N, where each element is an integer giving the predicted\r\n",
        "                class.\r\n",
        "        \"\"\"\r\n",
        "        # TODO: implement me\r\n",
        "        y = self.w.dot(X_test.T)\r\n",
        "        y_pred = np.argmax(y, axis=0)\r\n",
        "        y_pred = y_pred.reshape((len(y_pred),1))\r\n",
        "        return y_pred\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za51Kukzk5hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3597bf23-1a8b-4711-f472-7fe336d83666"
      },
      "source": [
        "lr = 0.01\n",
        "n_epochs = 10\n",
        "reg_const = 0.001\n",
        "\n",
        "softmax_CIFAR = Softmax(n_class_CIFAR, lr, n_epochs, reg_const)\n",
        "softmax_CIFAR.train(X_train_CIFAR, y_train_CIFAR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "weights:  [[ 1.77706627  0.33480156 -0.16365109 ...  0.92502039 -1.00383399\n",
            "  -0.13722045]\n",
            " [ 0.56878529  1.72280791  1.81602927 ...  0.90680201 -0.7301351\n",
            "  -1.95709743]\n",
            " [-0.67948992  2.11990772 -0.88324979 ... -1.88190067 -0.38278089\n",
            "  -1.87318037]\n",
            " ...\n",
            " [ 0.71251437 -0.43394002 -0.43857448 ... -0.62785216  0.54391818\n",
            "  -0.94684324]\n",
            " [ 1.30934016 -0.08950338  0.63694483 ...  1.03237856 -1.20543429\n",
            "  -0.71745508]\n",
            " [-0.87920542  1.56399134 -0.49234303 ... -1.70154434 -0.31455848\n",
            "   0.01411296]]\n",
            "\n",
            "changed z [-4253.16125056 -1363.57118122     0.         ... -6159.47816209\n",
            " -8379.93936617 -3137.36541679]\n",
            "\n",
            "p =  [[0.00000000e+000 0.00000000e+000 1.00000000e+000 ... 0.00000000e+000\n",
            "  0.00000000e+000 0.00000000e+000]\n",
            " [9.62467742e-249 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
            "  0.00000000e+000 0.00000000e+000]\n",
            " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
            "  0.00000000e+000 0.00000000e+000]\n",
            " ...\n",
            " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
            "  1.00000000e+000 0.00000000e+000]\n",
            " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
            "  0.00000000e+000 0.00000000e+000]\n",
            " [1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
            "  0.00000000e+000 0.00000000e+000]] [1. 1. 1. ... 1. 1. 1.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss in epoch 1 is inf\n",
            "\n",
            "weights:  [[-16.0309155   -3.0511426    1.43404678 ...  -8.31861539   9.04008318\n",
            "    1.23989987]\n",
            " [ -5.09548311 -15.48176439 -16.32060523 ...  -8.10067995   6.63246878\n",
            "   17.67467558]\n",
            " [  6.12227749 -19.07216947   7.95649262 ...  16.9225588    3.43079181\n",
            "   16.84505248]\n",
            " ...\n",
            " [ -6.39859099   3.91860829   3.95904279 ...   5.65490243  -4.89066551\n",
            "    8.52716486]\n",
            " [-11.74556555   0.84560175  -5.69074125 ...  -9.33555861  10.80616685\n",
            "    6.41568873]\n",
            " [  7.93080333 -14.05840392   4.44877652 ...  15.31194304   2.83035132\n",
            "   -0.1268253 ]]\n",
            "\n",
            "changed z [-51633.54562697 -48762.7228001  -68974.51972475 ... -16555.69184412\n",
            "  -8623.65077133 -44844.49266204]\n",
            "\n",
            "p =  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] [1. 1. 1. ... 1. 1. 1.]\n",
            "\n",
            "Loss in epoch 2 is inf\n",
            "\n",
            "weights:  [[ 144.33899768   27.52062605  -12.84521487 ...   74.90497508\n",
            "   -81.32293551  -11.1204224 ]\n",
            " [  45.83635127  139.31347331  146.86391506 ...   72.85854365\n",
            "   -59.73908145 -159.11791255]\n",
            " [ -55.11725944  171.63362013  -71.6240358  ... -152.30677246\n",
            "   -30.88235189 -151.61205338]\n",
            " ...\n",
            " [  57.58436475  -35.27112269  -35.63437902 ...  -50.90977957\n",
            "    44.00113551  -76.75823032]\n",
            " [ 105.69869956   -7.62401432   51.20051649 ...   84.06576702\n",
            "   -97.20874479  -57.6935619 ]\n",
            " [ -71.3234641   126.57970715  -39.98554091 ... -137.78437357\n",
            "   -25.4490339     1.16569432]]\n",
            "\n",
            "changed z [-356134.15982601  -93441.74376234       0.         ... -509370.36906494\n",
            " -702463.1048641  -310683.98471546]\n",
            "\n",
            "p =  [[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]] [1. 1. 1. ... 1. 1. 1.]\n",
            "\n",
            "Loss in epoch 3 is inf\n",
            "\n",
            "weights:  [[-1299.08327153  -247.71858225   115.57300519 ...  -674.13566469\n",
            "    731.91452264   100.09124842]\n",
            " [ -412.49880801 -1253.79317027 -1321.74704177 ...  -655.66302554\n",
            "    537.71638663  1432.12550373]\n",
            " [  496.06145788 -1544.69620235   644.62287832 ...  1370.74674132\n",
            "    277.92722726  1364.49517475]\n",
            " ...\n",
            " [ -518.24658199   317.45199789   320.72013721 ...   458.19108168\n",
            "   -396.00681089   690.82838829]\n",
            " [ -951.24738131    68.65855958  -460.76049768 ...  -756.63605209\n",
            "    874.83604368   519.20078744]\n",
            " [  641.93177938 -1139.19717459   359.89024768 ...  1240.05817265\n",
            "    229.04129331   -10.49045466]]\n",
            "\n",
            "changed z [-4183192.64462376 -3948638.055349   -5716217.66398506 ...\n",
            " -1302852.92546268  -662081.55689675 -3774430.86063413]\n",
            "\n",
            "p =  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] [1. 1. 1. ... 1. 1. 1.]\n",
            "\n",
            "Loss in epoch 4 is inf\n",
            "\n",
            "weights:  [[ 11691.81101418   2229.52839747  -1040.09501784 ...   6067.25904935\n",
            "   -6587.1923105    -900.78200525]\n",
            " [  3712.46589732  11284.11576353  11895.70151201 ...   5900.91929866\n",
            "   -4839.49466292 -12889.17569136]\n",
            " [ -4464.56959957  13902.25022041  -5801.62124011 ... -12336.72432002\n",
            "   -2501.3502101  -12280.46311911]\n",
            " ...\n",
            " [  4664.21665167  -2857.07126926  -2886.48391666 ...  -4123.7350715\n",
            "    3564.04676216  -6217.46893394]\n",
            " [  8561.21554437   -617.94014888   4146.82880841 ...   6809.77024043\n",
            "   -7873.47760608  -4672.7593638 ]\n",
            " [ -5777.33222449  10252.82866733  -3238.95872945 ... -11160.50043106\n",
            "   -2061.34751145     94.43834243]]\n",
            "\n",
            "changed z [-28857142.08327373  -7551892.87935347         0.         ...\n",
            " -41266707.3364964  -56920651.57996909 -25238356.11054384]\n",
            "\n",
            "p =  [[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]] [1. 1. 1. ... 1. 1. 1.]\n",
            "\n",
            "Loss in epoch 5 is inf\n",
            "\n",
            "weights:  [[-105226.33122044  -20065.78833926    9360.8214201  ...  -54605.32222181\n",
            "    59284.73901179    8107.04561247]\n",
            " [ -33412.164637   -101557.01369842 -107061.28533385 ...  -53108.20981558\n",
            "    43555.51661195  116002.64549593]\n",
            " [  40181.13253986 -125120.24558627   52214.59773512 ...  111030.50468438\n",
            "    22512.13796585  110524.15477543]\n",
            " ...\n",
            " [ -41977.9372185    25713.65325856   25978.36591502 ...   37113.61871527\n",
            "   -32076.41744325   55957.2247317 ]\n",
            " [ -77050.89893556    5561.5038244   -37321.4150731  ...  -61287.97632809\n",
            "    70861.25578096   42054.79298308]\n",
            " [  51996.01060291  -92275.43783944   29150.64891559 ...  100444.50268831\n",
            "    18552.12758725    -849.94429394]]\n",
            "\n",
            "changed z [-3.38839384e+08 -3.19838643e+08 -4.63144523e+08 ... -1.05492713e+08\n",
            " -5.35918812e+07 -3.05872581e+08]\n",
            "\n",
            "p =  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] [1. 1. 1. ... 1. 1. 1.]\n",
            "\n",
            "Loss in epoch 6 is inf\n",
            "\n",
            "weights:  [[  947037.04255213   180592.1562136    -84247.33074576 ...\n",
            "    491447.93806365  -533562.61271217   -72963.37128148]\n",
            " [  300709.45833972   914013.1004987    963551.5461225  ...\n",
            "    477973.84039806  -391999.69670207 -1044023.85563333]\n",
            " [ -361630.20933491  1126082.19467805  -469931.39494872 ...\n",
            "   -999274.5458018   -202609.24685185  -994717.39951928]\n",
            " ...\n",
            " [  377801.43239881  -231422.88259716  -233805.29589881 ...\n",
            "   -334022.58376286   288687.74246464  -503615.03601249]\n",
            " [  693458.07954514   -50053.54751786   335892.72000585 ...\n",
            "    551591.83272798  -637751.25523413  -378493.08911268]\n",
            " [ -467964.04163746   830478.99464932  -262355.78674269 ...\n",
            "   -904000.50108876  -166969.12417394     7649.52287987]]\n",
            "\n",
            "changed z [-2.33743870e+09 -6.11686426e+08  0.00000000e+00 ... -3.34261093e+09\n",
            " -4.61059383e+09 -2.04438021e+09]\n",
            "\n",
            "p =  [[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]] [1. 1. 1. ... 1. 1. 1.]\n",
            "\n",
            "Loss in epoch 7 is inf\n",
            "\n",
            "weights:  [[ -8523333.41506195  -1625329.4386844     758225.94297137 ...\n",
            "   -4423031.43335048   4802063.52262682    656670.34909856]\n",
            " [ -2706385.09661854  -8226117.87631494  -8671963.88682823 ...\n",
            "   -4301764.49971012   3527997.33496433   9396214.76497361]\n",
            " [  3254671.89015794 -10134739.74570499   4229382.56111258 ...\n",
            "    8993470.89802037   1823483.20774163   8952456.58237698]\n",
            " ...\n",
            " [ -3400212.87894277   2082805.95520965   2104247.6737544  ...\n",
            "    3006203.2569375   -2598189.67876561   4532535.32843869]\n",
            " [ -6241122.67494248    450481.97014523  -3023034.43585006 ...\n",
            "   -4964326.53871606   5739761.25443338   3406437.76072294]\n",
            " [  4211676.39531965  -7474310.93167737   2361202.10103474 ...\n",
            "    8136004.50860764   1502722.11754971    -68845.70513083]]\n",
            "\n",
            "changed z [-2.74459909e+10 -2.59069290e+10 -3.75148372e+10 ... -8.54487138e+09\n",
            " -4.34090564e+09 -2.47758227e+10]\n",
            "\n",
            "p =  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] [1. 1. 1. ... 1. 1. 1.]\n",
            "\n",
            "Loss in epoch 8 is inf\n",
            "\n",
            "weights:  [[ 76710000.79712568  14627965.00931983  -6824033.42470724 ...\n",
            "   39807282.93822168 -43218571.66524745  -5910033.10265626]\n",
            " [ 24357465.84617354  74035060.86404735  78047674.95957191 ...\n",
            "   38715880.44944888 -31751976.06187352 -84565932.93093242]\n",
            " [-29292047.02789761  91212657.69574651 -38064443.06534588 ...\n",
            "  -80941238.08582568 -16411348.87483392 -80572109.24793333]\n",
            " ...\n",
            " [ 30601915.90791727 -18745253.60015699 -18938229.06645325 ...\n",
            "  -27055829.32776293  23383707.09436585 -40792817.96937547]\n",
            " [ 56170104.06360742  -4054337.74440536  27207309.9069985  ...\n",
            "   44678938.89421965 -51657851.24310587 -30657939.79877149]\n",
            " [-37905087.50408805  67268798.43919079 -21250818.85581509 ...\n",
            "  -73224040.55436276 -13524499.03383612    619611.37041191]]\n",
            "\n",
            "changed z [-1.89332545e+11 -4.95465836e+10  0.00000000e+00 ... -2.70751493e+11\n",
            " -3.73458121e+11 -1.65594870e+11]\n",
            "\n",
            "p =  [[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]] [1. 1. 1. ... 1. 1. 1.]\n",
            "\n",
            "Loss in epoch 9 is inf\n",
            "\n",
            "weights:  [[-6.90390007e+08 -1.31651685e+08  6.14163008e+07 ... -3.58265546e+08\n",
            "   3.88967145e+08  5.31902979e+07]\n",
            " [-2.19217193e+08 -6.66315548e+08 -7.02429075e+08 ... -3.48442924e+08\n",
            "   2.85767785e+08  7.61093396e+08]\n",
            " [ 2.63628423e+08 -8.20913919e+08  3.42579988e+08 ...  7.28471143e+08\n",
            "   1.47702140e+08  7.25148983e+08]\n",
            " ...\n",
            " [-2.75417243e+08  1.68707282e+08  1.70444062e+08 ...  2.43502464e+08\n",
            "  -2.10453364e+08  3.67135362e+08]\n",
            " [-5.05530937e+08  3.64890397e+07 -2.44865789e+08 ... -4.02110450e+08\n",
            "   4.64920661e+08  2.75921458e+08]\n",
            " [ 3.41145788e+08 -6.05419186e+08  1.91257370e+08 ...  6.59016365e+08\n",
            "   1.21720491e+08 -5.57650233e+06]]\n",
            "\n",
            "changed z [-2.22312526e+12 -2.09846125e+12 -3.03870195e+12 ... -6.92134544e+11\n",
            " -3.51613320e+11 -2.00684179e+12]\n",
            "\n",
            "p =  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] [1. 1. 1. ... 1. 1. 1.]\n",
            "\n",
            "Loss in epoch 10 is inf\n",
            "[[ 6.21351006e+09  1.18486517e+09 -5.52746707e+08 ...  3.22438992e+09\n",
            "  -3.50070430e+09 -4.78712681e+08]\n",
            " [ 1.97295473e+09  5.99683993e+09  6.32186167e+09 ...  3.13598632e+09\n",
            "  -2.57191006e+09 -6.84984057e+09]\n",
            " [-2.37265581e+09  7.38822527e+09 -3.08321989e+09 ... -6.55624028e+09\n",
            "  -1.32931926e+09 -6.52634085e+09]\n",
            " ...\n",
            " [ 2.47875519e+09 -1.51836554e+09 -1.53399655e+09 ... -2.19152218e+09\n",
            "   1.89408027e+09 -3.30421826e+09]\n",
            " [ 4.54977843e+09 -3.28401358e+08  2.20379210e+09 ...  3.61899405e+09\n",
            "  -4.18428595e+09 -2.48329312e+09]\n",
            " [-3.07031209e+09  5.44877267e+09 -1.72131633e+09 ... -5.93114728e+09\n",
            "  -1.09548442e+09  5.01885210e+07]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkRlZEvmk5hb"
      },
      "source": [
        "pred_softmax = softmax_CIFAR.predict(X_train_CIFAR)\n",
        "print('The training accuracy is given by: %f' % (get_acc(pred_softmax, y_train_CIFAR)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5T1NWtnk5hc"
      },
      "source": [
        "### Validate Softmax on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1YWo9Ctk5hc"
      },
      "source": [
        "pred_softmax = softmax_CIFAR.predict(X_val_CIFAR)\n",
        "print('The validation accuracy is given by: %f' % (get_acc(pred_softmax, y_val_CIFAR)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-KGCF-Tk5hc"
      },
      "source": [
        "### Testing Softmax on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNEH_j2uk5hc"
      },
      "source": [
        "pred_softmax = softmax_CIFAR.predict(X_test_CIFAR)\n",
        "print('The testing accuracy is given by: %f' % (get_acc(pred_softmax, y_test_CIFAR)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ2mwGnvk5hd"
      },
      "source": [
        "## Train Softmax on Mushroom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzYy4CugSQRF"
      },
      "source": [
        "\"\"\"Softmax model.\"\"\"\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "class Softmax:\r\n",
        "    def __init__(self, n_class: int, lr: float, epochs: int, reg_const: float):\r\n",
        "        \"\"\"Initialize a new classifier.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            n_class: the number of classes\r\n",
        "            lr: the learning rate\r\n",
        "            epochs: the number of epochs to train for\r\n",
        "            reg_const: the regularization constant\r\n",
        "        \"\"\"\r\n",
        "        self.lr = lr\r\n",
        "        self.epochs = epochs\r\n",
        "        self.reg_const = reg_const\r\n",
        "        self.n_class = n_class\r\n",
        "        self.w = np.random.randn(self.n_class,22) * 0.1\r\n",
        "\r\n",
        "    def calc_gradient(self, X_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\r\n",
        "        \"\"\"Calculate gradient of the softmax loss.\r\n",
        "\r\n",
        "        Inputs have dimension D, there are C classes, and we operate on\r\n",
        "        mini-batches of N examples.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            X_train: a numpy array of shape (N, D) containing a mini-batch\r\n",
        "                of data\r\n",
        "            y_train: a numpy array of shape (N,) containing training labels;\r\n",
        "                y[i] = c means that X[i] has label c, where 0 <= c < C\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            gradient with respect to weights w; an array of same shape as w\r\n",
        "        \"\"\"\r\n",
        "        #Calculating Cross entropy loss\r\n",
        "        # print(\"\\nweights: \",self.w)\r\n",
        "        x = X_train\r\n",
        "        z = np.dot(self.w, x) # 2x22 - 22x4874 = 2x4874(z)\r\n",
        "        # print(\"\\noriginal z\", z)\r\n",
        "        z -= np.max(z, axis=0) \r\n",
        "        # print(\"\\nchanged z\",z)\r\n",
        "        p = np.exp(z) / np.sum(np.exp(z), axis=0) # Softmax function 2x4874(p)\r\n",
        "        log_likelihood = -np.log10(p[y_train, range(len(y_train))])\r\n",
        "        L = np.sum(log_likelihood)/len(y_train) #Cross entropy loss\r\n",
        "        R = 0.5 * np.sum(np.multiply(self.w, self.w))\r\n",
        "        loss = L + R * reg_const\r\n",
        "\r\n",
        "        #Calculation of Grad\r\n",
        "        p[y_train, range(len(y_train))] -= 1\r\n",
        "        dW = (1 / len(y_train)) * p.dot(x.T) + (reg_const * self.w)\r\n",
        "        return loss, dW\r\n",
        "\r\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\r\n",
        "        \"\"\"Train the classifier.\r\n",
        "\r\n",
        "        Hint: operate on mini-batches of data for SGD.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            X_train: a numpy array of shape (N, D) containing training data;\r\n",
        "                N examples with D dimensions\r\n",
        "            y_train: a numpy array of shape (N,) containing training labels\r\n",
        "        \"\"\"\r\n",
        "        x = X_train.T\r\n",
        "        N = x.shape[1]\r\n",
        "        loss_calc = []\r\n",
        "        for itr in range(1, self.epochs+1):\r\n",
        "          loss, grad = self.calc_gradient(x, y_train)\r\n",
        "          loss_calc.append(loss)\r\n",
        "          print(f\"\\nLoss in epoch {itr} is {loss}\")\r\n",
        "          # print(f\"\\nGrad in epoch {itr} is {grad}\")\r\n",
        "          self.w -= self.lr * grad \r\n",
        "        print(self.w)\r\n",
        "        return\r\n",
        "\r\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\r\n",
        "        \"\"\"Use the trained weights to predict labels for test data points.\r\n",
        "\r\n",
        "        Parameters:\r\n",
        "            X_test: a numpy array of shape (N, D) containing testing data;\r\n",
        "                N examples with D dimensions\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            predicted labels for the data in X_test; a 1-dimensional array of\r\n",
        "                length N, where each element is an integer giving the predicted\r\n",
        "                class.\r\n",
        "        \"\"\"\r\n",
        "        # TODO: implement me\r\n",
        "        y = self.w.dot(X_test.T)\r\n",
        "        y_pred = np.argmax(y, axis=0)\r\n",
        "        y_pred = y_pred.reshape((len(y_pred),1))\r\n",
        "        return y_pred\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH29MKCuk5hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b42cedfb-7b9e-4ff5-95bb-364b1d8a26f8"
      },
      "source": [
        "lr = 0.00001\n",
        "n_epochs = 500\n",
        "reg_const = 5000\n",
        "\n",
        "softmax_MR = Softmax(n_class_MR, lr, n_epochs, reg_const)\n",
        "softmax_MR.train(X_train_MR, y_train_MR)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss in epoch 1 is 3296.543893039342\n",
            "\n",
            "Loss in epoch 2 is 3127.070816054008\n",
            "\n",
            "Loss in epoch 3 is 2972.9899792232586\n",
            "\n",
            "Loss in epoch 4 is 2832.9436582764515\n",
            "\n",
            "Loss in epoch 5 is 2705.690948979557\n",
            "\n",
            "Loss in epoch 6 is 2590.0975267568747\n",
            "\n",
            "Loss in epoch 7 is 2485.126413221451\n",
            "\n",
            "Loss in epoch 8 is 2389.8296346994607\n",
            "\n",
            "Loss in epoch 9 is 2303.3406701988733\n",
            "\n",
            "Loss in epoch 10 is 2224.867597996586\n",
            "\n",
            "Loss in epoch 11 is 2153.686861002505\n",
            "\n",
            "Loss in epoch 12 is 2089.137581212795\n",
            "\n",
            "Loss in epoch 13 is 2030.6163628100696\n",
            "\n",
            "Loss in epoch 14 is 1977.5725317569959\n",
            "\n",
            "Loss in epoch 15 is 1929.5037670417264\n",
            "\n",
            "Loss in epoch 16 is 1885.952085082797\n",
            "\n",
            "Loss in epoch 17 is 1846.5001442295895\n",
            "\n",
            "Loss in epoch 18 is 1810.767840868561\n",
            "\n",
            "Loss in epoch 19 is 1778.4091724494485\n",
            "\n",
            "Loss in epoch 20 is 1749.1093458724386\n",
            "\n",
            "Loss in epoch 21 is 1722.5821122260468\n",
            "\n",
            "Loss in epoch 22 is 1698.5673109304432\n",
            "\n",
            "Loss in epoch 23 is 1676.8286080129303\n",
            "\n",
            "Loss in epoch 24 is 1657.1514146011907\n",
            "\n",
            "Loss in epoch 25 is 1639.3409728354557\n",
            "\n",
            "Loss in epoch 26 is 1623.2205973318491\n",
            "\n",
            "Loss in epoch 27 is 1608.630061122551\n",
            "\n",
            "Loss in epoch 28 is 1595.4241156923124\n",
            "\n",
            "Loss in epoch 29 is 1583.4711353524092\n",
            "\n",
            "Loss in epoch 30 is 1572.6518767648065\n",
            "\n",
            "Loss in epoch 31 is 1562.8583449637188\n",
            "\n",
            "Loss in epoch 32 is 1553.9927577309847\n",
            "\n",
            "Loss in epoch 33 is 1545.9666006697498\n",
            "\n",
            "Loss in epoch 34 is 1538.699765792631\n",
            "\n",
            "Loss in epoch 35 is 1532.119766897156\n",
            "\n",
            "Loss in epoch 36 is 1526.161025442493\n",
            "\n",
            "Loss in epoch 37 is 1520.7642210676916\n",
            "\n",
            "Loss in epoch 38 is 1515.8757013018385\n",
            "\n",
            "Loss in epoch 39 is 1511.4469454090145\n",
            "\n",
            "Loss in epoch 40 is 1507.4340776864565\n",
            "\n",
            "Loss in epoch 41 is 1503.797425890391\n",
            "\n",
            "Loss in epoch 42 is 1500.5011208012386\n",
            "\n",
            "Loss in epoch 43 is 1497.5127332580344\n",
            "\n",
            "Loss in epoch 44 is 1494.8029452899873\n",
            "\n",
            "Loss in epoch 45 is 1492.3452522525602\n",
            "\n",
            "Loss in epoch 46 is 1490.115693135672\n",
            "\n",
            "Loss in epoch 47 is 1488.0926064536166\n",
            "\n",
            "Loss in epoch 48 is 1486.2564093506767\n",
            "\n",
            "Loss in epoch 49 is 1484.589397763656\n",
            "\n",
            "Loss in epoch 50 is 1483.0755656740191\n",
            "\n",
            "Loss in epoch 51 is 1481.700441658271\n",
            "\n",
            "Loss in epoch 52 is 1480.4509411069237\n",
            "\n",
            "Loss in epoch 53 is 1479.3152326309498\n",
            "\n",
            "Loss in epoch 54 is 1478.2826173099631\n",
            "\n",
            "Loss in epoch 55 is 1477.343419561019\n",
            "\n",
            "Loss in epoch 56 is 1476.4888885198877\n",
            "\n",
            "Loss in epoch 57 is 1475.7111089301916\n",
            "\n",
            "Loss in epoch 58 is 1475.002920629898\n",
            "\n",
            "Loss in epoch 59 is 1474.3578458104323\n",
            "\n",
            "Loss in epoch 60 is 1473.7700233014455\n",
            "\n",
            "Loss in epoch 61 is 1473.2341492052628\n",
            "\n",
            "Loss in epoch 62 is 1472.7454232692053\n",
            "\n",
            "Loss in epoch 63 is 1472.299500442294\n",
            "\n",
            "Loss in epoch 64 is 1471.8924471158068\n",
            "\n",
            "Loss in epoch 65 is 1471.5207015951457\n",
            "\n",
            "Loss in epoch 66 is 1471.1810383937104\n",
            "\n",
            "Loss in epoch 67 is 1470.8705359790818\n",
            "\n",
            "Loss in epoch 68 is 1470.586547637218\n",
            "\n",
            "Loss in epoch 69 is 1470.326675152691\n",
            "\n",
            "Loss in epoch 70 is 1470.0887450321193\n",
            "\n",
            "Loss in epoch 71 is 1469.8707870243466\n",
            "\n",
            "Loss in epoch 72 is 1469.6710147146662\n",
            "\n",
            "Loss in epoch 73 is 1469.4878079921114\n",
            "\n",
            "Loss in epoch 74 is 1469.3196972081507\n",
            "\n",
            "Loss in epoch 75 is 1469.1653488627833\n",
            "\n",
            "Loss in epoch 76 is 1469.0235526699994\n",
            "\n",
            "Loss in epoch 77 is 1468.8932098688947\n",
            "\n",
            "Loss in epoch 78 is 1468.7733226595847\n",
            "\n",
            "Loss in epoch 79 is 1468.66298465508\n",
            "\n",
            "Loss in epoch 80 is 1468.5613722505514\n",
            "\n",
            "Loss in epoch 81 is 1468.467736821236\n",
            "\n",
            "Loss in epoch 82 is 1468.381397668654\n",
            "\n",
            "Loss in epoch 83 is 1468.3017356427556\n",
            "\n",
            "Loss in epoch 84 is 1468.2281873746297\n",
            "\n",
            "Loss in epoch 85 is 1468.1602400606655\n",
            "\n",
            "Loss in epoch 86 is 1468.097426744879\n",
            "\n",
            "Loss in epoch 87 is 1468.0393220513681\n",
            "\n",
            "Loss in epoch 88 is 1467.985538323257\n",
            "\n",
            "Loss in epoch 89 is 1467.9357221291134\n",
            "\n",
            "Loss in epoch 90 is 1467.8895511012577\n",
            "\n",
            "Loss in epoch 91 is 1467.8467310740364\n",
            "\n",
            "Loss in epoch 92 is 1467.8069934932034\n",
            "\n",
            "Loss in epoch 93 is 1467.7700930702695\n",
            "\n",
            "Loss in epoch 94 is 1467.735805658359\n",
            "\n",
            "Loss in epoch 95 is 1467.7039263281752\n",
            "\n",
            "Loss in epoch 96 is 1467.6742676250283\n",
            "\n",
            "Loss in epoch 97 is 1467.6466579894502\n",
            "\n",
            "Loss in epoch 98 is 1467.6209403258017\n",
            "\n",
            "Loss in epoch 99 is 1467.5969707046763\n",
            "\n",
            "Loss in epoch 100 is 1467.5746171863566\n",
            "\n",
            "Loss in epoch 101 is 1467.5537587537742\n",
            "\n",
            "Loss in epoch 102 is 1467.5342843444716\n",
            "\n",
            "Loss in epoch 103 is 1467.516091972289\n",
            "\n",
            "Loss in epoch 104 is 1467.4990879300633\n",
            "\n",
            "Loss in epoch 105 is 1467.483186065832\n",
            "\n",
            "Loss in epoch 106 is 1467.4683071254522\n",
            "\n",
            "Loss in epoch 107 is 1467.4543781554298\n",
            "\n",
            "Loss in epoch 108 is 1467.4413319602409\n",
            "\n",
            "Loss in epoch 109 is 1467.429106609026\n",
            "\n",
            "Loss in epoch 110 is 1467.4176449869753\n",
            "\n",
            "Loss in epoch 111 is 1467.406894387278\n",
            "\n",
            "Loss in epoch 112 is 1467.396806139805\n",
            "\n",
            "Loss in epoch 113 is 1467.3873352730327\n",
            "\n",
            "Loss in epoch 114 is 1467.3784402062117\n",
            "\n",
            "Loss in epoch 115 is 1467.3700824689126\n",
            "\n",
            "Loss in epoch 116 is 1467.36222644539\n",
            "\n",
            "Loss in epoch 117 is 1467.3548391414815\n",
            "\n",
            "Loss in epoch 118 is 1467.3478899720346\n",
            "\n",
            "Loss in epoch 119 is 1467.3413505668148\n",
            "\n",
            "Loss in epoch 120 is 1467.3351945933516\n",
            "\n",
            "Loss in epoch 121 is 1467.3293975951121\n",
            "\n",
            "Loss in epoch 122 is 1467.3239368435513\n",
            "\n",
            "Loss in epoch 123 is 1467.318791202893\n",
            "\n",
            "Loss in epoch 124 is 1467.3139410063827\n",
            "\n",
            "Loss in epoch 125 is 1467.3093679430215\n",
            "\n",
            "Loss in epoch 126 is 1467.3050549538518\n",
            "\n",
            "Loss in epoch 127 is 1467.300986136904\n",
            "\n",
            "Loss in epoch 128 is 1467.297146660058\n",
            "\n",
            "Loss in epoch 129 is 1467.29352268111\n",
            "\n",
            "Loss in epoch 130 is 1467.2901012743703\n",
            "\n",
            "Loss in epoch 131 is 1467.2868703633021\n",
            "\n",
            "Loss in epoch 132 is 1467.2838186585855\n",
            "\n",
            "Loss in epoch 133 is 1467.2809356011526\n",
            "\n",
            "Loss in epoch 134 is 1467.278211309807\n",
            "\n",
            "Loss in epoch 135 is 1467.275636532997\n",
            "\n",
            "Loss in epoch 136 is 1467.2732026043539\n",
            "\n",
            "Loss in epoch 137 is 1467.2709014017419\n",
            "\n",
            "Loss in epoch 138 is 1467.268725309463\n",
            "\n",
            "Loss in epoch 139 is 1467.266667183405\n",
            "\n",
            "Loss in epoch 140 is 1467.2647203188\n",
            "\n",
            "Loss in epoch 141 is 1467.262878420493\n",
            "\n",
            "Loss in epoch 142 is 1467.2611355753822\n",
            "\n",
            "Loss in epoch 143 is 1467.2594862269718\n",
            "\n",
            "Loss in epoch 144 is 1467.257925151765\n",
            "\n",
            "Loss in epoch 145 is 1467.2564474374237\n",
            "\n",
            "Loss in epoch 146 is 1467.25504846252\n",
            "\n",
            "Loss in epoch 147 is 1467.2537238776954\n",
            "\n",
            "Loss in epoch 148 is 1467.2524695882505\n",
            "\n",
            "Loss in epoch 149 is 1467.2512817379247\n",
            "\n",
            "Loss in epoch 150 is 1467.250156693822\n",
            "\n",
            "Loss in epoch 151 is 1467.2490910323943\n",
            "\n",
            "Loss in epoch 152 is 1467.2480815264094\n",
            "\n",
            "Loss in epoch 153 is 1467.2471251328125\n",
            "\n",
            "Loss in epoch 154 is 1467.2462189813878\n",
            "\n",
            "Loss in epoch 155 is 1467.2453603642227\n",
            "\n",
            "Loss in epoch 156 is 1467.2445467258672\n",
            "\n",
            "Loss in epoch 157 is 1467.2437756541497\n",
            "\n",
            "Loss in epoch 158 is 1467.2430448715859\n",
            "\n",
            "Loss in epoch 159 is 1467.2423522273562\n",
            "\n",
            "Loss in epoch 160 is 1467.2416956898455\n",
            "\n",
            "Loss in epoch 161 is 1467.2410733395893\n",
            "\n",
            "Loss in epoch 162 is 1467.240483362738\n",
            "\n",
            "Loss in epoch 163 is 1467.2399240448888\n",
            "\n",
            "Loss in epoch 164 is 1467.239393765364\n",
            "\n",
            "Loss in epoch 165 is 1467.238890991785\n",
            "\n",
            "Loss in epoch 166 is 1467.2384142750207\n",
            "\n",
            "Loss in epoch 167 is 1467.2379622444278\n",
            "\n",
            "Loss in epoch 168 is 1467.2375336034183\n",
            "\n",
            "Loss in epoch 169 is 1467.237127125272\n",
            "\n",
            "Loss in epoch 170 is 1467.236741649172\n",
            "\n",
            "Loss in epoch 171 is 1467.2363760765602\n",
            "\n",
            "Loss in epoch 172 is 1467.2360293676254\n",
            "\n",
            "Loss in epoch 173 is 1467.2357005380397\n",
            "\n",
            "Loss in epoch 174 is 1467.2353886559265\n",
            "\n",
            "Loss in epoch 175 is 1467.235092838899\n",
            "\n",
            "Loss in epoch 176 is 1467.2348122514077\n",
            "\n",
            "Loss in epoch 177 is 1467.2345461021187\n",
            "\n",
            "Loss in epoch 178 is 1467.234293641522\n",
            "\n",
            "Loss in epoch 179 is 1467.2340541596711\n",
            "\n",
            "Loss in epoch 180 is 1467.2338269839886\n",
            "\n",
            "Loss in epoch 181 is 1467.2336114772777\n",
            "\n",
            "Loss in epoch 182 is 1467.2334070358238\n",
            "\n",
            "Loss in epoch 183 is 1467.2332130875243\n",
            "\n",
            "Loss in epoch 184 is 1467.2330290902867\n",
            "\n",
            "Loss in epoch 185 is 1467.232854530333\n",
            "\n",
            "Loss in epoch 186 is 1467.23268892074\n",
            "\n",
            "Loss in epoch 187 is 1467.2325317999735\n",
            "\n",
            "Loss in epoch 188 is 1467.2323827305806\n",
            "\n",
            "Loss in epoch 189 is 1467.2322412978508\n",
            "\n",
            "Loss in epoch 190 is 1467.2321071086487\n",
            "\n",
            "Loss in epoch 191 is 1467.2319797902744\n",
            "\n",
            "Loss in epoch 192 is 1467.2318589893587\n",
            "\n",
            "Loss in epoch 193 is 1467.231744370873\n",
            "\n",
            "Loss in epoch 194 is 1467.2316356171334\n",
            "\n",
            "Loss in epoch 195 is 1467.2315324269043\n",
            "\n",
            "Loss in epoch 196 is 1467.2314345145546\n",
            "\n",
            "Loss in epoch 197 is 1467.2313416091952\n",
            "\n",
            "Loss in epoch 198 is 1467.2312534539449\n",
            "\n",
            "Loss in epoch 199 is 1467.231169805167\n",
            "\n",
            "Loss in epoch 200 is 1467.2310904318172\n",
            "\n",
            "Loss in epoch 201 is 1467.2310151147544\n",
            "\n",
            "Loss in epoch 202 is 1467.23094364612\n",
            "\n",
            "Loss in epoch 203 is 1467.2308758287775\n",
            "\n",
            "Loss in epoch 204 is 1467.2308114757243\n",
            "\n",
            "Loss in epoch 205 is 1467.230750409593\n",
            "\n",
            "Loss in epoch 206 is 1467.2306924621214\n",
            "\n",
            "Loss in epoch 207 is 1467.2306374737138\n",
            "\n",
            "Loss in epoch 208 is 1467.2305852929635\n",
            "\n",
            "Loss in epoch 209 is 1467.2305357762368\n",
            "\n",
            "Loss in epoch 210 is 1467.2304887872801\n",
            "\n",
            "Loss in epoch 211 is 1467.2304441968458\n",
            "\n",
            "Loss in epoch 212 is 1467.2304018822888\n",
            "\n",
            "Loss in epoch 213 is 1467.2303617272773\n",
            "\n",
            "Loss in epoch 214 is 1467.230323621428\n",
            "\n",
            "Loss in epoch 215 is 1467.2302874600307\n",
            "\n",
            "Loss in epoch 216 is 1467.2302531437324\n",
            "\n",
            "Loss in epoch 217 is 1467.230220578261\n",
            "\n",
            "Loss in epoch 218 is 1467.2301896741906\n",
            "\n",
            "Loss in epoch 219 is 1467.2301603466528\n",
            "\n",
            "Loss in epoch 220 is 1467.2301325151486\n",
            "\n",
            "Loss in epoch 221 is 1467.23010610328\n",
            "\n",
            "Loss in epoch 222 is 1467.230081038555\n",
            "\n",
            "Loss in epoch 223 is 1467.230057252202\n",
            "\n",
            "Loss in epoch 224 is 1467.2300346789696\n",
            "\n",
            "Loss in epoch 225 is 1467.2300132569214\n",
            "\n",
            "Loss in epoch 226 is 1467.2299929273158\n",
            "\n",
            "Loss in epoch 227 is 1467.2299736343905\n",
            "\n",
            "Loss in epoch 228 is 1467.2299553252487\n",
            "\n",
            "Loss in epoch 229 is 1467.2299379496822\n",
            "\n",
            "Loss in epoch 230 is 1467.2299214600673\n",
            "\n",
            "Loss in epoch 231 is 1467.2299058112028\n",
            "\n",
            "Loss in epoch 232 is 1467.2298909601832\n",
            "\n",
            "Loss in epoch 233 is 1467.2298768663213\n",
            "\n",
            "Loss in epoch 234 is 1467.229863490988\n",
            "\n",
            "Loss in epoch 235 is 1467.2298507975324\n",
            "\n",
            "Loss in epoch 236 is 1467.2298387511742\n",
            "\n",
            "Loss in epoch 237 is 1467.2298273189149\n",
            "\n",
            "Loss in epoch 238 is 1467.2298164694296\n",
            "\n",
            "Loss in epoch 239 is 1467.229806173\n",
            "\n",
            "Loss in epoch 240 is 1467.2297964014203\n",
            "\n",
            "Loss in epoch 241 is 1467.2297871279363\n",
            "\n",
            "Loss in epoch 242 is 1467.2297783271374\n",
            "\n",
            "Loss in epoch 243 is 1467.2297699749302\n",
            "\n",
            "Loss in epoch 244 is 1467.229762048438\n",
            "\n",
            "Loss in epoch 245 is 1467.229754525953\n",
            "\n",
            "Loss in epoch 246 is 1467.2297473868773\n",
            "\n",
            "Loss in epoch 247 is 1467.2297406116675\n",
            "\n",
            "Loss in epoch 248 is 1467.2297341817734\n",
            "\n",
            "Loss in epoch 249 is 1467.2297280795894\n",
            "\n",
            "Loss in epoch 250 is 1467.229722288403\n",
            "\n",
            "Loss in epoch 251 is 1467.2297167923682\n",
            "\n",
            "Loss in epoch 252 is 1467.2297115764375\n",
            "\n",
            "Loss in epoch 253 is 1467.2297066263325\n",
            "\n",
            "Loss in epoch 254 is 1467.2297019284983\n",
            "\n",
            "Loss in epoch 255 is 1467.2296974700846\n",
            "\n",
            "Loss in epoch 256 is 1467.2296932388826\n",
            "\n",
            "Loss in epoch 257 is 1467.2296892233073\n",
            "\n",
            "Loss in epoch 258 is 1467.229685412371\n",
            "\n",
            "Loss in epoch 259 is 1467.2296817956476\n",
            "\n",
            "Loss in epoch 260 is 1467.2296783632307\n",
            "\n",
            "Loss in epoch 261 is 1467.22967510574\n",
            "\n",
            "Loss in epoch 262 is 1467.2296720142433\n",
            "\n",
            "Loss in epoch 263 is 1467.2296690802932\n",
            "\n",
            "Loss in epoch 264 is 1467.22966629585\n",
            "\n",
            "Loss in epoch 265 is 1467.2296636533047\n",
            "\n",
            "Loss in epoch 266 is 1467.2296611454185\n",
            "\n",
            "Loss in epoch 267 is 1467.229658765331\n",
            "\n",
            "Loss in epoch 268 is 1467.2296565065203\n",
            "\n",
            "Loss in epoch 269 is 1467.2296543628186\n",
            "\n",
            "Loss in epoch 270 is 1467.2296523283615\n",
            "\n",
            "Loss in epoch 271 is 1467.2296503975635\n",
            "\n",
            "Loss in epoch 272 is 1467.2296485651625\n",
            "\n",
            "Loss in epoch 273 is 1467.2296468261313\n",
            "\n",
            "Loss in epoch 274 is 1467.229645175718\n",
            "\n",
            "Loss in epoch 275 is 1467.2296436093998\n",
            "\n",
            "Loss in epoch 276 is 1467.2296421228996\n",
            "\n",
            "Loss in epoch 277 is 1467.229640712144\n",
            "\n",
            "Loss in epoch 278 is 1467.2296393732752\n",
            "\n",
            "Loss in epoch 279 is 1467.2296381026283\n",
            "\n",
            "Loss in epoch 280 is 1467.2296368967302\n",
            "\n",
            "Loss in epoch 281 is 1467.2296357522775\n",
            "\n",
            "Loss in epoch 282 is 1467.2296346661396\n",
            "\n",
            "Loss in epoch 283 is 1467.2296336353502\n",
            "\n",
            "Loss in epoch 284 is 1467.2296326570765\n",
            "\n",
            "Loss in epoch 285 is 1467.229631728664\n",
            "\n",
            "Loss in epoch 286 is 1467.229630847547\n",
            "\n",
            "Loss in epoch 287 is 1467.2296300113358\n",
            "\n",
            "Loss in epoch 288 is 1467.2296292177282\n",
            "\n",
            "Loss in epoch 289 is 1467.2296284645597\n",
            "\n",
            "Loss in epoch 290 is 1467.2296277497642\n",
            "\n",
            "Loss in epoch 291 is 1467.2296270713946\n",
            "\n",
            "Loss in epoch 292 is 1467.2296264275922\n",
            "\n",
            "Loss in epoch 293 is 1467.2296258165884\n",
            "\n",
            "Loss in epoch 294 is 1467.2296252367246\n",
            "\n",
            "Loss in epoch 295 is 1467.2296246864057\n",
            "\n",
            "Loss in epoch 296 is 1467.2296241641297\n",
            "\n",
            "Loss in epoch 297 is 1467.2296236684633\n",
            "\n",
            "Loss in epoch 298 is 1467.2296231980504\n",
            "\n",
            "Loss in epoch 299 is 1467.229622751611\n",
            "\n",
            "Loss in epoch 300 is 1467.2296223279138\n",
            "\n",
            "Loss in epoch 301 is 1467.2296219258105\n",
            "\n",
            "Loss in epoch 302 is 1467.2296215441932\n",
            "\n",
            "Loss in epoch 303 is 1467.2296211820212\n",
            "\n",
            "Loss in epoch 304 is 1467.2296208383034\n",
            "\n",
            "Loss in epoch 305 is 1467.2296205121008\n",
            "\n",
            "Loss in epoch 306 is 1467.2296202025163\n",
            "\n",
            "Loss in epoch 307 is 1467.2296199087136\n",
            "\n",
            "Loss in epoch 308 is 1467.229619629873\n",
            "\n",
            "Loss in epoch 309 is 1467.2296193652428\n",
            "\n",
            "Loss in epoch 310 is 1467.2296191140895\n",
            "\n",
            "Loss in epoch 311 is 1467.229618875744\n",
            "\n",
            "Loss in epoch 312 is 1467.2296186495391\n",
            "\n",
            "Loss in epoch 313 is 1467.229618434861\n",
            "\n",
            "Loss in epoch 314 is 1467.2296182311188\n",
            "\n",
            "Loss in epoch 315 is 1467.2296180377605\n",
            "\n",
            "Loss in epoch 316 is 1467.2296178542497\n",
            "\n",
            "Loss in epoch 317 is 1467.2296176800926\n",
            "\n",
            "Loss in epoch 318 is 1467.229617514812\n",
            "\n",
            "Loss in epoch 319 is 1467.2296173579457\n",
            "\n",
            "Loss in epoch 320 is 1467.2296172090746\n",
            "\n",
            "Loss in epoch 321 is 1467.2296170677987\n",
            "\n",
            "Loss in epoch 322 is 1467.22961693371\n",
            "\n",
            "Loss in epoch 323 is 1467.2296168064527\n",
            "\n",
            "Loss in epoch 324 is 1467.2296166856884\n",
            "\n",
            "Loss in epoch 325 is 1467.2296165710677\n",
            "\n",
            "Loss in epoch 326 is 1467.2296164622958\n",
            "\n",
            "Loss in epoch 327 is 1467.229616359063\n",
            "\n",
            "Loss in epoch 328 is 1467.2296162610905\n",
            "\n",
            "Loss in epoch 329 is 1467.229616168108\n",
            "\n",
            "Loss in epoch 330 is 1467.2296160798699\n",
            "\n",
            "Loss in epoch 331 is 1467.2296159961174\n",
            "\n",
            "Loss in epoch 332 is 1467.2296159166387\n",
            "\n",
            "Loss in epoch 333 is 1467.229615841203\n",
            "\n",
            "Loss in epoch 334 is 1467.2296157696196\n",
            "\n",
            "Loss in epoch 335 is 1467.2296157016783\n",
            "\n",
            "Loss in epoch 336 is 1467.2296156371997\n",
            "\n",
            "Loss in epoch 337 is 1467.2296155760089\n",
            "\n",
            "Loss in epoch 338 is 1467.2296155179317\n",
            "\n",
            "Loss in epoch 339 is 1467.2296154628182\n",
            "\n",
            "Loss in epoch 340 is 1467.2296154105106\n",
            "\n",
            "Loss in epoch 341 is 1467.2296153608668\n",
            "\n",
            "Loss in epoch 342 is 1467.2296153137572\n",
            "\n",
            "Loss in epoch 343 is 1467.2296152690437\n",
            "\n",
            "Loss in epoch 344 is 1467.229615226609\n",
            "\n",
            "Loss in epoch 345 is 1467.2296151863347\n",
            "\n",
            "Loss in epoch 346 is 1467.229615148113\n",
            "\n",
            "Loss in epoch 347 is 1467.2296151118444\n",
            "\n",
            "Loss in epoch 348 is 1467.2296150774189\n",
            "\n",
            "Loss in epoch 349 is 1467.22961504475\n",
            "\n",
            "Loss in epoch 350 is 1467.229615013741\n",
            "\n",
            "Loss in epoch 351 is 1467.2296149843185\n",
            "\n",
            "Loss in epoch 352 is 1467.2296149563856\n",
            "\n",
            "Loss in epoch 353 is 1467.2296149298857\n",
            "\n",
            "Loss in epoch 354 is 1467.229614904731\n",
            "\n",
            "Loss in epoch 355 is 1467.2296148808575\n",
            "\n",
            "Loss in epoch 356 is 1467.2296148582045\n",
            "\n",
            "Loss in epoch 357 is 1467.2296148367038\n",
            "\n",
            "Loss in epoch 358 is 1467.2296148163023\n",
            "\n",
            "Loss in epoch 359 is 1467.2296147969332\n",
            "\n",
            "Loss in epoch 360 is 1467.2296147785535\n",
            "\n",
            "Loss in epoch 361 is 1467.2296147611123\n",
            "\n",
            "Loss in epoch 362 is 1467.2296147445577\n",
            "\n",
            "Loss in epoch 363 is 1467.2296147288455\n",
            "\n",
            "Loss in epoch 364 is 1467.2296147139366\n",
            "\n",
            "Loss in epoch 365 is 1467.2296146997883\n",
            "\n",
            "Loss in epoch 366 is 1467.2296146863553\n",
            "\n",
            "Loss in epoch 367 is 1467.229614673609\n",
            "\n",
            "Loss in epoch 368 is 1467.229614661516\n",
            "\n",
            "Loss in epoch 369 is 1467.2296146500353\n",
            "\n",
            "Loss in epoch 370 is 1467.2296146391418\n",
            "\n",
            "Loss in epoch 371 is 1467.2296146288043\n",
            "\n",
            "Loss in epoch 372 is 1467.2296146189897\n",
            "\n",
            "Loss in epoch 373 is 1467.229614609676\n",
            "\n",
            "Loss in epoch 374 is 1467.2296146008389\n",
            "\n",
            "Loss in epoch 375 is 1467.2296145924538\n",
            "\n",
            "Loss in epoch 376 is 1467.22961458449\n",
            "\n",
            "Loss in epoch 377 is 1467.2296145769358\n",
            "\n",
            "Loss in epoch 378 is 1467.2296145697694\n",
            "\n",
            "Loss in epoch 379 is 1467.2296145629591\n",
            "\n",
            "Loss in epoch 380 is 1467.229614556502\n",
            "\n",
            "Loss in epoch 381 is 1467.2296145503763\n",
            "\n",
            "Loss in epoch 382 is 1467.2296145445557\n",
            "\n",
            "Loss in epoch 383 is 1467.229614539038\n",
            "\n",
            "Loss in epoch 384 is 1467.2296145338003\n",
            "\n",
            "Loss in epoch 385 is 1467.2296145288271\n",
            "\n",
            "Loss in epoch 386 is 1467.2296145241085\n",
            "\n",
            "Loss in epoch 387 is 1467.229614519628\n",
            "\n",
            "Loss in epoch 388 is 1467.2296145153791\n",
            "\n",
            "Loss in epoch 389 is 1467.229614511346\n",
            "\n",
            "Loss in epoch 390 is 1467.2296145075195\n",
            "\n",
            "Loss in epoch 391 is 1467.2296145038824\n",
            "\n",
            "Loss in epoch 392 is 1467.2296145004368\n",
            "\n",
            "Loss in epoch 393 is 1467.229614497164\n",
            "\n",
            "Loss in epoch 394 is 1467.2296144940597\n",
            "\n",
            "Loss in epoch 395 is 1467.2296144911109\n",
            "\n",
            "Loss in epoch 396 is 1467.229614488312\n",
            "\n",
            "Loss in epoch 397 is 1467.2296144856598\n",
            "\n",
            "Loss in epoch 398 is 1467.229614483139\n",
            "\n",
            "Loss in epoch 399 is 1467.2296144807485\n",
            "\n",
            "Loss in epoch 400 is 1467.2296144784793\n",
            "\n",
            "Loss in epoch 401 is 1467.2296144763263\n",
            "\n",
            "Loss in epoch 402 is 1467.2296144742782\n",
            "\n",
            "Loss in epoch 403 is 1467.2296144723393\n",
            "\n",
            "Loss in epoch 404 is 1467.2296144705037\n",
            "\n",
            "Loss in epoch 405 is 1467.229614468756\n",
            "\n",
            "Loss in epoch 406 is 1467.229614467099\n",
            "\n",
            "Loss in epoch 407 is 1467.2296144655265\n",
            "\n",
            "Loss in epoch 408 is 1467.229614464031\n",
            "\n",
            "Loss in epoch 409 is 1467.2296144626118\n",
            "\n",
            "Loss in epoch 410 is 1467.2296144612671\n",
            "\n",
            "Loss in epoch 411 is 1467.2296144599916\n",
            "\n",
            "Loss in epoch 412 is 1467.2296144587792\n",
            "\n",
            "Loss in epoch 413 is 1467.2296144576326\n",
            "\n",
            "Loss in epoch 414 is 1467.2296144565355\n",
            "\n",
            "Loss in epoch 415 is 1467.2296144554984\n",
            "\n",
            "Loss in epoch 416 is 1467.2296144545219\n",
            "\n",
            "Loss in epoch 417 is 1467.229614453584\n",
            "\n",
            "Loss in epoch 418 is 1467.2296144526981\n",
            "\n",
            "Loss in epoch 419 is 1467.2296144518634\n",
            "\n",
            "Loss in epoch 420 is 1467.2296144510663\n",
            "\n",
            "Loss in epoch 421 is 1467.2296144503064\n",
            "\n",
            "Loss in epoch 422 is 1467.2296144495906\n",
            "\n",
            "Loss in epoch 423 is 1467.2296144489087\n",
            "\n",
            "Loss in epoch 424 is 1467.2296144482602\n",
            "\n",
            "Loss in epoch 425 is 1467.22961444765\n",
            "\n",
            "Loss in epoch 426 is 1467.229614447066\n",
            "\n",
            "Loss in epoch 427 is 1467.229614446513\n",
            "\n",
            "Loss in epoch 428 is 1467.229614445983\n",
            "\n",
            "Loss in epoch 429 is 1467.22961444549\n",
            "\n",
            "Loss in epoch 430 is 1467.2296144450208\n",
            "\n",
            "Loss in epoch 431 is 1467.2296144445668\n",
            "\n",
            "Loss in epoch 432 is 1467.229614444141\n",
            "\n",
            "Loss in epoch 433 is 1467.229614443739\n",
            "\n",
            "Loss in epoch 434 is 1467.2296144433587\n",
            "\n",
            "Loss in epoch 435 is 1467.2296144429922\n",
            "\n",
            "Loss in epoch 436 is 1467.2296144426448\n",
            "\n",
            "Loss in epoch 437 is 1467.2296144423124\n",
            "\n",
            "Loss in epoch 438 is 1467.229614442007\n",
            "\n",
            "Loss in epoch 439 is 1467.2296144417148\n",
            "\n",
            "Loss in epoch 440 is 1467.2296144414277\n",
            "\n",
            "Loss in epoch 441 is 1467.2296144411625\n",
            "\n",
            "Loss in epoch 442 is 1467.2296144409156\n",
            "\n",
            "Loss in epoch 443 is 1467.2296144406662\n",
            "\n",
            "Loss in epoch 444 is 1467.2296144404438\n",
            "\n",
            "Loss in epoch 445 is 1467.229614440233\n",
            "\n",
            "Loss in epoch 446 is 1467.2296144400273\n",
            "\n",
            "Loss in epoch 447 is 1467.2296144398315\n",
            "\n",
            "Loss in epoch 448 is 1467.229614439647\n",
            "\n",
            "Loss in epoch 449 is 1467.2296144394759\n",
            "\n",
            "Loss in epoch 450 is 1467.229614439309\n",
            "\n",
            "Loss in epoch 451 is 1467.2296144391496\n",
            "\n",
            "Loss in epoch 452 is 1467.2296144390016\n",
            "\n",
            "Loss in epoch 453 is 1467.2296144388565\n",
            "\n",
            "Loss in epoch 454 is 1467.2296144387258\n",
            "\n",
            "Loss in epoch 455 is 1467.2296144385946\n",
            "\n",
            "Loss in epoch 456 is 1467.2296144384761\n",
            "\n",
            "Loss in epoch 457 is 1467.2296144383627\n",
            "\n",
            "Loss in epoch 458 is 1467.2296144382576\n",
            "\n",
            "Loss in epoch 459 is 1467.2296144381437\n",
            "\n",
            "Loss in epoch 460 is 1467.2296144380398\n",
            "\n",
            "Loss in epoch 461 is 1467.2296144379497\n",
            "\n",
            "Loss in epoch 462 is 1467.2296144378686\n",
            "\n",
            "Loss in epoch 463 is 1467.2296144377726\n",
            "\n",
            "Loss in epoch 464 is 1467.229614437703\n",
            "\n",
            "Loss in epoch 465 is 1467.2296144376244\n",
            "\n",
            "Loss in epoch 466 is 1467.2296144375437\n",
            "\n",
            "Loss in epoch 467 is 1467.229614437491\n",
            "\n",
            "Loss in epoch 468 is 1467.2296144374186\n",
            "\n",
            "Loss in epoch 469 is 1467.2296144373493\n",
            "\n",
            "Loss in epoch 470 is 1467.2296144372997\n",
            "\n",
            "Loss in epoch 471 is 1467.2296144372358\n",
            "\n",
            "Loss in epoch 472 is 1467.2296144371935\n",
            "\n",
            "Loss in epoch 473 is 1467.2296144371437\n",
            "\n",
            "Loss in epoch 474 is 1467.2296144370957\n",
            "\n",
            "Loss in epoch 475 is 1467.2296144370412\n",
            "\n",
            "Loss in epoch 476 is 1467.2296144370157\n",
            "\n",
            "Loss in epoch 477 is 1467.2296144369743\n",
            "\n",
            "Loss in epoch 478 is 1467.2296144369323\n",
            "\n",
            "Loss in epoch 479 is 1467.2296144368954\n",
            "\n",
            "Loss in epoch 480 is 1467.2296144368534\n",
            "\n",
            "Loss in epoch 481 is 1467.2296144368142\n",
            "\n",
            "Loss in epoch 482 is 1467.2296144368079\n",
            "\n",
            "Loss in epoch 483 is 1467.229614436758\n",
            "\n",
            "Loss in epoch 484 is 1467.229614436746\n",
            "\n",
            "Loss in epoch 485 is 1467.229614436696\n",
            "\n",
            "Loss in epoch 486 is 1467.229614436689\n",
            "\n",
            "Loss in epoch 487 is 1467.2296144366842\n",
            "\n",
            "Loss in epoch 488 is 1467.2296144366323\n",
            "\n",
            "Loss in epoch 489 is 1467.2296144366014\n",
            "\n",
            "Loss in epoch 490 is 1467.2296144365832\n",
            "\n",
            "Loss in epoch 491 is 1467.229614436569\n",
            "\n",
            "Loss in epoch 492 is 1467.2296144365498\n",
            "\n",
            "Loss in epoch 493 is 1467.2296144365287\n",
            "\n",
            "Loss in epoch 494 is 1467.2296144365184\n",
            "\n",
            "Loss in epoch 495 is 1467.229614436518\n",
            "\n",
            "Loss in epoch 496 is 1467.2296144365052\n",
            "\n",
            "Loss in epoch 497 is 1467.2296144364618\n",
            "\n",
            "Loss in epoch 498 is 1467.2296144364711\n",
            "\n",
            "Loss in epoch 499 is 1467.2296144364782\n",
            "\n",
            "Loss in epoch 500 is 1467.2296144364145\n",
            "[[ 3.36746000e-04  1.81411572e-04  4.50492409e-04  4.18752568e-05\n",
            "   4.10976611e-04  9.76200263e-05  1.55108732e-05  3.11858850e-05\n",
            "   4.79790727e-04  5.65449322e-05  1.11222814e-04  1.58022157e-04\n",
            "   1.60504718e-04  5.79441937e-04  5.77656955e-04 -4.32460949e-13\n",
            "   1.96553139e-04  1.06565449e-04  2.29688140e-04  3.56052524e-04\n",
            "   3.66064834e-04  1.51661879e-04]\n",
            " [ 3.36746000e-04  1.81411573e-04  4.50492409e-04  4.18752572e-05\n",
            "   4.10976612e-04  9.76200239e-05  1.55108749e-05  3.11858851e-05\n",
            "   4.79790726e-04  5.65449320e-05  1.11222815e-04  1.58022159e-04\n",
            "   1.60504719e-04  5.79441937e-04  5.77656955e-04 -1.04187256e-12\n",
            "   1.96553139e-04  1.06565450e-04  2.29688140e-04  3.56052524e-04\n",
            "   3.66064832e-04  1.51661878e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMTDV1sAk5hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2af149a-5bad-46d3-abac-72babe682716"
      },
      "source": [
        "pred_softmax = softmax_MR.predict(X_train_MR)\n",
        "print('The training accuracy is given by: %f' % (get_acc(pred_softmax, y_train_MR)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test [[1]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [0]] pred [[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [1]\n",
            " [0]\n",
            " [1]]\n",
            "The training accuracy is given by: 40.541650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBYw1__vk5he"
      },
      "source": [
        "### Validate Softmax on Mushroom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQW0Lg42k5he",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572e73a7-b1ba-4905-9397-776604acdddf"
      },
      "source": [
        "pred_softmax = softmax_MR.predict(X_val_MR)\n",
        "print('The validation accuracy is given by: %f' % (get_acc(pred_softmax, y_val_MR)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test [[1]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]] pred [[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n",
            "The validation accuracy is given by: 40.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpnO80kXk5he"
      },
      "source": [
        "### Testing Softmax on Mushroom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqUZj6Evk5he"
      },
      "source": [
        "pred_softmax = softmax_MR.predict(X_test_MR)\n",
        "print('The testing accuracy is given by: %f' % (get_acc(pred_softmax, y_test_MR)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}